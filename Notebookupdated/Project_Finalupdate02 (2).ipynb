{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daasWgi99j-g"
      },
      "outputs": [],
      "source": [
        "## !pip install datasets\n",
        "## !pip install python-dotenv\n",
        "##المكتبات"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "load_dotenv()\n",
        "#هنا سجلنا  في الموقع\n",
        "huggingface_T = os.getenv(\"huggingface_T\")\n",
        "login(token=huggingface_T)"
      ],
      "metadata": {
        "id": "Nr7Yy3nY_KK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"KFUPM-JRCAI/arabic-generated-abstracts\")\n",
        "# سوينا لود للداتا سيت\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "83mcS8eRBmsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_split(split, split_name):\n",
        "    \"\"\"\n",
        "    Convert one split into a unified dataframe with labels\n",
        "    \"\"\"\n",
        "    data = []\n",
        "\n",
        "    # Human abstracts\n",
        "    for text in split[\"original_abstract\"]:\n",
        "        data.append({\n",
        "            \"text\": text,\n",
        "            \"label\": 0,               # 0 = Human\n",
        "            \"generated_by\": \"human\",\n",
        "            \"source_split\": split_name\n",
        "        })\n",
        "\n",
        "    # AI abstracts\n",
        "    ai_sources = {\n",
        "        \"allam\": split[\"allam_generated_abstract\"],\n",
        "        \"jais\": split[\"jais_generated_abstract\"],\n",
        "        \"llama\": split[\"llama_generated_abstract\"],\n",
        "        \"openai\": split[\"openai_generated_abstract\"],\n",
        "    }\n",
        "\n",
        "    for model, texts in ai_sources.items():\n",
        "        for text in texts:\n",
        "            data.append({\n",
        "                \"text\": text,\n",
        "                \"label\": 1,           # 1 = AI\n",
        "                \"generated_by\": model,\n",
        "                \"source_split\": split_name\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "IAwwUp1I_Sax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#Process all splits\n",
        "df_by_polishing = process_split(dataset[\"by_polishing\"], \"by_polishing\")\n",
        "df_from_title = process_split(dataset[\"from_title\"], \"from_title\")\n",
        "df_from_title_and_content = process_split(\n",
        "    dataset[\"from_title_and_content\"],\n",
        "    \"from_title_and_content\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "dU2QWIjSIRIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge all splits\n",
        "final_df = pd.concat(\n",
        "    [df_by_polishing, df_from_title, df_from_title_and_content],\n",
        "    ignore_index=True\n",
        ")"
      ],
      "metadata": {
        "id": "gx5Qw52kIqIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head(5)"
      ],
      "metadata": {
        "id": "Fj4L0lVjIvFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "\n",
        "final_df.to_excel(\n",
        "    \"data/raw/all_raw_data.xlsx\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"Raw data saved as XLSX\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xMjpbV59T_PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = final_df.copy()\n",
        "\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "# عدد الصفوف المكررة بالكامل\n",
        "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "# عدد القيم المكررة في كل عمود\n",
        "for col in df.columns:\n",
        "    print(f\"Duplicates in column '{col}': {df[col].duplicated().sum()}\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "# عدد القيم الفارغة أو التي تحتوي مسافات فقط (مهم للنصوص)\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == \"object\":\n",
        "        empty_count = df[col].apply(\n",
        "            lambda x: isinstance(x, str) and x.strip() == \"\"\n",
        "        ).sum()\n",
        "        print(f\"Blank or space-only values in column '{col}': {empty_count}\")\n",
        "    else:\n",
        "        print(f\"Blank or space-only values in column '{col}': Not applicable\")\n",
        "\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(\"First phase ends here\")"
      ],
      "metadata": {
        "id": "0o_mgE7Kioup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from datasets import load_dataset\n",
        "#مكتبات نحتاجها لي البريبروسيسنق"
      ],
      "metadata": {
        "id": "k_Yqo5v7ixeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "72ddNwe_zp2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_diacritics(text):\n",
        "    diacritics = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    return re.sub(diacritics, '', text)\n",
        "\n",
        "def normalize(text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ؤ\", \"و\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ئ\", \"ي\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"[^؀-ۿ ]+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "stop_words = set(stopwords.words(\"arabic\"))\n",
        "stemmer = ISRIStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = normalize(text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    tokens = [stemmer.stem(w) for w in tokens]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "VDj5kEEULsAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = final_df.copy()\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "print(\"Preprocessing completed\")\n",
        "print(df.columns)\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "cFzU8kjsLs8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "\n",
        "clean_df.to_excel(\n",
        "    \"data/processed/all_clean_data.xlsx\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"Clean data saved as Excel (.xlsx) successfully!\")\n"
      ],
      "metadata": {
        "id": "ZCMFclTBUU2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Human_Texts = df[df[\"label\"] == 0][\"clean_text\"].dropna().tolist()\n",
        "AI_Texts = df[df[\"label\"] == 1][\"clean_text\"].dropna().tolist()"
      ],
      "metadata": {
        "id": "io2occuBLs_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def text_stats(texts):\n",
        "    words = [w for txt in texts for w in txt.split()]\n",
        "    avg_word_len = np.mean([len(w) for w in words])\n",
        "    avg_sent_len = np.mean([len(txt.split()) for txt in texts])\n",
        "    vocab = set(words)\n",
        "    CD = len(vocab) / len(words)\n",
        "    return avg_word_len, avg_sent_len, CD\n",
        "\n",
        "stats_human = text_stats(Human_Texts)\n",
        "stats_ai = text_stats(AI_Texts)\n",
        "\n",
        "print(\"\\nStatistical Summary:\")\n",
        "print(f\"Human: Avg word len={stats_human[0]:.2f}, \"\n",
        "      f\"Avg sent len={stats_human[1]:.2f}, CD={stats_human[2]:.3f}\")\n",
        "\n",
        "print(f\"AI: Avg word len={stats_ai[0]:.2f}, \"\n",
        "      f\"Avg sent len={stats_ai[1]:.2f}, CD={stats_ai[2]:.3f}\")"
      ],
      "metadata": {
        "id": "UU8Qc4u5LtCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer"
      ],
      "metadata": {
        "id": "ttIivnYMM1Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def plot_top_ngrams(texts, label_name, n=2, top_k=10):\n",
        "    vec = CountVectorizer(ngram_range=(n, n))\n",
        "    bag = vec.fit_transform(texts)\n",
        "    sum_words = bag.sum(axis=0)\n",
        "    freqs = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    freqs = sorted(freqs, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "    words, counts = zip(*freqs)\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.barplot(x=list(counts), y=list(words))\n",
        "    plt.title(f\"Top {top_k} {n}-grams – {label_name}\")\n",
        "    plt.show()\n",
        "\n",
        "plot_top_ngrams(Human_Texts, \"Human Abstracts\", n=2)\n",
        "plot_top_ngrams(AI_Texts, \"AI-generated Abstracts\", n=2)"
      ],
      "metadata": {
        "id": "Pkmj5TfLMsbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[df[\"label\"]==0][\"text_length\"], bins=30, label=\"Human\", alpha=0.6)\n",
        "sns.histplot(df[df[\"label\"]==1][\"text_length\"], bins=30, label=\"AI\", alpha=0.6)\n",
        "plt.xlabel(\"Text Length (words)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Text Length Distribution\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YCw-TdDCMsgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def type_token_ratio(text):\n",
        "    words = text.split()\n",
        "    return len(set(words)) / len(words) if words else 0\n",
        "\n",
        "df[\"TTR\"] = df[\"clean_text\"].apply(type_token_ratio)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.boxplot(\n",
        "    [df[df[\"label\"]==0][\"TTR\"], df[df[\"label\"]==1][\"TTR\"]],\n",
        "    labels=[\"Human\", \"AI\"]\n",
        ")\n",
        "plt.title(\"Vocabulary Richness (TTR)\")\n",
        "plt.ylabel(\"TTR Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1w1TpT8_Msif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "Human_words = \" \".join(df[df[\"label\"]==0][\"clean_text\"]).split()\n",
        "AI_words = \" \".join(df[df[\"label\"]==1][\"clean_text\"]).split()\n",
        "\n",
        "Human_freq = Counter(Human_words)\n",
        "AI_freq = Counter(AI_words)\n",
        "\n",
        "common_words = set(Human_freq.keys()) & set(AI_freq.keys())\n",
        "\n",
        "data = [(w, Human_freq[w], AI_freq[w]) for w in list(common_words)[:15]]\n",
        "\n",
        "freq_df = pd.DataFrame(data, columns=[\"word\", \"Human\", \"AI\"])\n",
        "\n",
        "freq_df.plot(\n",
        "    x=\"word\", kind=\"bar\", figsize=(10,5),\n",
        "    title=\"Top Common Words: Human vs AI\", rot=45\n",
        ")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tSBM_UeZNEOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZGU4No6HNERu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGxzuLdVNEVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "import regex as re2"
      ],
      "metadata": {
        "id": "kqN9ZaWDD8Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper functions\n",
        "\n",
        "\n",
        "def simple_word_tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenize text into words / symbols with Arabic support.\n",
        "    \"\"\"\n",
        "    return re2.findall(r\"\\p{Arabic}+|\\w+|[^\\s\\w]\", text, flags=re2.VERSION1)\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    \"\"\"\n",
        "    Split text into sentences using Arabic/English punctuation.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    parts = re.split(r'(?<=[\\.\\?\\!\\u061F\\u061B])\\s+', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "def paragraph_tokenize(text):\n",
        "    \"\"\"\n",
        "    Split text into paragraphs based on double newlines.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    paragraphs = re.split(r'\\s*\\n\\s*\\n\\s*|\\s*\\r\\n\\s*\\r\\n\\s*', text.strip())\n",
        "    return [p.strip() for p in paragraphs if p.strip()]\n"
      ],
      "metadata": {
        "id": "CbmjrFGakOd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column names to use\n",
        "original_text_columns = \"text\"\n",
        "clean_text_columns = \"clean_text\"\n",
        "\n",
        "\n",
        "#  Tokens  تستخرج من النص clean\n",
        "\n",
        "df[\"tokens\"] = df[clean_text_columns].apply(\n",
        "    lambda t: [tok for tok in simple_word_tokenize(t) if tok.strip()] if isinstance(t, str) else []\n",
        ")\n",
        "\n",
        "\n",
        "# Words تستخرج من النص clean\n",
        "\n",
        "df[\"words\"] = df[\"tokens\"].apply(\n",
        "    lambda toks: [tok for tok in toks if re.search(r'\\w', tok)]\n",
        ")\n",
        "\n",
        "\n",
        "# Sentences تستخرج من النص الاصلي\n",
        "\n",
        "df[\"sentences\"] = df[original_text_columns].apply(\n",
        "    lambda t: sentence_tokenize(t)\n",
        ")\n",
        "\n",
        "\n",
        "#  Paragraphs تستخرج من النص الاصلي\n",
        "\n",
        "df[\"paragraphs\"] = df[original_text_columns].apply(\n",
        "    lambda t: paragraph_tokenize(t)\n",
        ")\n",
        "\n",
        "print(\"Feature engineering completed! Columns now:\")\n",
        "print(df.columns)\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "id": "vadNtQXLkOic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 14: Hapax dislegomena Ratio-يعطي مؤشرًا على تكرار الكلمات النادرة.\n",
        "feature_name = f'{clean_text_columns}_f014_hapax_dislegomena_ratio'\n",
        "\n",
        "def _hapax_dislegomena_ratio(words):\n",
        "    \"\"\"Calculates Hapax Dislegomena Ratio (V2/N).\"\"\"\n",
        "    N = len(words)\n",
        "    if N == 0:\n",
        "        return 0.0\n",
        "\n",
        "    c = Counter([w.lower() for w in words])\n",
        "    # V2 is the number of words that appear exactly twice\n",
        "    V2 = sum(1 for v in c.values() if v == 2)\n",
        "\n",
        "    return float(V2) / N\n",
        "\n",
        "df[feature_name] = df[\"words\"].apply(_hapax_dislegomena_ratio)"
      ],
      "metadata": {
        "id": "X3hMlnJAkOkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35. Total number of paragraphs (P)\n",
        "df['f035_Total_number_of_paragraphs_(P)'] = df[\"paragraphs\"].apply(len)"
      ],
      "metadata": {
        "id": "Kj5ZmL8Qm2jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (37) Average number of words /P\n",
        "df['f037_Average_words_per_paragraph'] = df.apply(\n",
        "    lambda row: len(row['words']) / row['f035_Total_number_of_paragraphs_(P)']\n",
        "    if row['f035_Total_number_of_paragraphs_(P)'] > 0 else 0,\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "xQPfprXQkOna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "uPqiL4T-kOqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 60: Number of words found in top 1000 positions (approx using global freq)\n",
        "from collections import Counter\n",
        "\n",
        "# 1) حساب التردد العالمي من كل أعمدة الكلمات\n",
        "all_words = []\n",
        "for col in original_text_columns:\n",
        "    words_col = f\"{col}_words\"\n",
        "    if words_col in df.columns:\n",
        "        for ws in df[words_col]:\n",
        "            all_words.extend([w.lower() for w in ws])\n",
        "\n",
        "global_freq = Counter(all_words)\n",
        "_top = [w for w, _ in global_freq.most_common()]  # ترتيب الكلمات من الأكثر تكراراً للأقل\n",
        "\n",
        "# 2) دالة لحساب الكلمات الموجودة ضمن أعلى k = 1000 كلمة\n",
        "def _count_top(ws, k=1000):\n",
        "    return sum(1 for w in set([x.lower() for x in ws]) if w in _top[:k])\n",
        "\n",
        "# 3) تطبيق الميزة لكل عمود نص\n",
        "for col in original_text_columns:\n",
        "    words_col = f\"{col}_words\"\n",
        "    if words_col in df.columns:\n",
        "        df[f'{col}_f060_num_words_in_top1000'] = df[words_col].apply(_count_top)"
      ],
      "metadata": {
        "id": "LER9KXY1nAqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 83: Type-Token Ratio (TTR)\n",
        "def compute_ttr(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 0.0\n",
        "\n",
        "    tokens = simple_word_tokenize(text)\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "\n",
        "    unique_tokens = {w.lower() for w in tokens}\n",
        "    return len(unique_tokens) / len(tokens)\n",
        "\n",
        "\n",
        "# Apply Feature 83\n",
        "clean_text_columns = [\"clean_text\"]  # لازم List\n",
        "\n",
        "for col in clean_text_columns:\n",
        "    df[f\"{col}_f083_ttr\"] = df[col].apply(compute_ttr)\n"
      ],
      "metadata": {
        "id": "DV8effqtqfM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 106: Tanween Frequency (Nunation)\n",
        "feature_name = f\"{original_text_columns}_f106_tanween_frequency\"\n",
        "\n",
        "def tanween_frequency(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return 0.0\n",
        "\n",
        "    tokens = simple_word_tokenize(text)\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "\n",
        "    tanween_chars = {\"ً\", \"ٌ\", \"ٍ\"}\n",
        "\n",
        "    tanween_count = sum(\n",
        "        1 for tok in tokens for ch in tok if ch in tanween_chars\n",
        "    )\n",
        "\n",
        "    return tanween_count / len(tokens)\n",
        "\n",
        "\n",
        "# Apply Feature 106 on original_text_columns\n",
        "df[feature_name] = df[original_text_columns].apply(tanween_frequency)"
      ],
      "metadata": {
        "id": "6xNiJW2_ui9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "lFJr6vrwyGA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42, shuffle=True)\n",
        "# First split: Train 70%, Temp 30%\n",
        "\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42, shuffle=True)\n",
        "# Second split: Temp 30% → 15% Validation, 15% Test\n",
        "\n",
        "print(\"TOTAL:\", len(df))\n",
        "print(\"TRAIN:\", len(train_df))\n",
        "print(\"VAL:\", len(val_df))\n",
        "print(\"TEST:\", len(test_df))\n",
        "# Show sizes"
      ],
      "metadata": {
        "id": "X_rTra7wv_3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply with clean_text only\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer for Arabic text\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,   # limit vocabulary\n",
        "    ngram_range=(1, 2),  # unigrams + bigrams\n",
        "    analyzer='word'\n",
        ")\n",
        "\n",
        "# Fit only on training set\n",
        "tfidf_vectorizer.fit(train_df[\"clean_text\"])\n",
        "\n",
        "# Transform train/validation/test sets\n",
        "X_train_tfidf = tfidf_vectorizer.transform(train_df[\"clean_text\"])\n",
        "X_val_tfidf   = tfidf_vectorizer.transform(val_df[\"clean_text\"])\n",
        "X_test_tfidf  = tfidf_vectorizer.transform(test_df[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF shapes:\")\n",
        "print(\"Train:\", X_train_tfidf.shape)\n",
        "print(\"Validation:\", X_val_tfidf.shape)\n",
        "print(\"Test:\", X_test_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "xInT_kV0ypJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)\n"
      ],
      "metadata": {
        "id": "p0a62p19zaSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack\n",
        "# Select numeric features (The generated feature engineering exclude label and text)\n",
        "EXCLUDED_COLS = [\n",
        "    'label',\n",
        "    'text',\n",
        "    'clean_text',\n",
        "    'tokens',\n",
        "    'words',\n",
        "    'sentences',\n",
        "    'paragraphs'\n",
        "]\n",
        "\n",
        "# Select columns that are numeric AND not in the exclusion list >> feature engineering columns\n",
        "numeric_cols = [\n",
        "    col for col in train_df.select_dtypes(include=np.number).columns.tolist()\n",
        "    if col not in EXCLUDED_COLS\n",
        "]\n",
        "\n",
        "# Convert the numeric features DataFrames to NumPy arrays (dense matrices)\n",
        "# We must use the values/to_numpy() method to extract the array for sparse matrix stacking.\n",
        "X_train_num_array = train_df[numeric_cols].values\n",
        "X_val_num_array   = val_df[numeric_cols].values\n",
        "X_test_num_array  = test_df[numeric_cols].values\n",
        "\n",
        "\n",
        "# Target variable\n",
        "y_train = train_df[\"label\"]\n",
        "y_val   = val_df[\"label\"]\n",
        "y_test  = test_df[\"label\"]\n",
        "\n",
        "# Features: TF-IDF and the creating feature engineering\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X_train = hstack([X_train_tfidf, X_train_num_array]).tocsr()\n",
        "X_val   = hstack([X_val_tfidf, X_val_num_array]).tocsr()\n",
        "X_test  = hstack([X_test_tfidf, X_test_num_array]).tocsr()\n",
        "\n",
        "print(\"X and y are ready for ML models.\")\n",
        "print(\"Train:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation:\", X_val.shape, y_val.shape)\n",
        "print(\"Test:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "aMf2-WSZz-oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(numeric_cols))\n",
        "print(numeric_cols)"
      ],
      "metadata": {
        "id": "3dG-eL0t1BiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize the model\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train on training set\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred = lr_model.predict(X_val)\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(y_val, y_val_pred))"
      ],
      "metadata": {
        "id": "DqLeTEd-1Eee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "# Predict on test set\n",
        "y_test_pred = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "# Optional: confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZlSYgW9K1P6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Dictionary to store models and results\n",
        "models = {}\n",
        "\n",
        "# -----------------------\n",
        "#Support Vector Machine (SVM)\n",
        "# -----------------------\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred_svm = svm_model.predict(X_val)\n",
        "print(\"SVM Validation Accuracy:\", accuracy_score(y_val, y_val_pred_svm))\n",
        "print(classification_report(y_val, y_val_pred_svm))\n",
        "\n",
        "models['SVM'] = svm_model\n",
        "\n",
        "# -----------------------\n",
        "#Random Forest\n",
        "# -----------------------\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred_rf = rf_model.predict(X_val)\n",
        "print(\"Random Forest Validation Accuracy:\", accuracy_score(y_val, y_val_pred_rf))\n",
        "print(classification_report(y_val, y_val_pred_rf))\n",
        "\n",
        "models['RandomForest'] = rf_model\n",
        "\n",
        "# -----------------------\n",
        "#XGBoost\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred_xgb = xgb_model.predict(X_val)\n",
        "print(\"XGBoost Validation Accuracy:\", accuracy_score(y_val, y_val_pred_xgb))\n",
        "print(classification_report(y_val, y_val_pred_xgb))\n",
        "\n",
        "models['XGBoost'] = xgb_model"
      ],
      "metadata": {
        "id": "dYyys-rY1UK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "models_list = ['SVM', 'RandomForest', 'XGBoost']\n",
        "\n",
        "for model_name in models_list:\n",
        "\n",
        "    current_model = models[model_name]\n",
        "    test_predictions = current_model.predict(X_test)\n",
        "\n",
        "    print(f\"\\n--- Evaluation on Test Set: {model_name} ---\")\n",
        "    print(\"Accuracy Score:\", accuracy_score(y_test, test_predictions))\n",
        "    print(\"\\nDetailed Classification Results:\")\n",
        "    print(classification_report(y_test, test_predictions))\n",
        "\n",
        "    matrix = confusion_matrix(y_test, test_predictions)\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Test Confusion Matrix ({model_name})')\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('True Class')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "GJh95GbR4M8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "o9k0oMuICH-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "embedding_model = SentenceTransformer(\n",
        "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "\n",
        "train_texts = train_df[\"clean_text\"].tolist()\n",
        "val_texts   = val_df[\"clean_text\"].tolist()\n",
        "test_texts  = test_df[\"clean_text\"].tolist()\n",
        "\n",
        "X_train_emb = embedding_model.encode(train_texts, convert_to_numpy=True)\n",
        "X_val_emb   = embedding_model.encode(val_texts, convert_to_numpy=True)\n",
        "X_test_emb  = embedding_model.encode(test_texts, convert_to_numpy=True)\n",
        "\n",
        "y_train = train_df[\"label\"].to_numpy()\n",
        "y_val   = val_df[\"label\"].to_numpy()\n",
        "y_test  = test_df[\"label\"].to_numpy()\n",
        "\n",
        "print(\"Training embeddings shape:\", X_train_emb.shape)\n"
      ],
      "metadata": {
        "id": "-cpckVC4CLbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "classifier = models.Sequential()\n",
        "\n",
        "classifier.add(layers.InputLayer(input_shape=(X_train_emb.shape[1],)))\n",
        "classifier.add(layers.Dense(256, activation=\"relu\"))\n",
        "classifier.add(layers.Dropout(0.3))\n",
        "classifier.add(layers.Dense(128, activation=\"relu\"))\n",
        "classifier.add(layers.Dropout(0.3))\n",
        "classifier.add(layers.Dense(1, activation=\"sigmoid\"))  # Binary classification output\n",
        "\n",
        "classifier.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "classifier.summary()\n"
      ],
      "metadata": {
        "id": "RX_y75F2Ckh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_history = classifier.fit(\n",
        "    X_train_emb,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_emb, y_val)\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZT-9SAiOCyON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Generate predictions on the test set\n",
        "test_probs = classifier.predict(X_test_emb)\n",
        "test_preds = (test_probs >= 0.5).astype(int)\n",
        "\n",
        "print(\"Test Set Accuracy:\", accuracy_score(y_test, test_preds))\n",
        "print(\"\\nClassification Results:\")\n",
        "print(classification_report(y_test, test_preds))\n"
      ],
      "metadata": {
        "id": "geEwrOwFCzw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "from tensorflow.keras.models import Model as KerasModel\n",
        "\n",
        "def persist_models(model_collection, output_path=\"models\"):\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    for key, mdl in model_collection.items():\n",
        "\n",
        "        if isinstance(mdl, KerasModel):\n",
        "            save_path = os.path.join(output_path, f\"{key}.h5\")\n",
        "            mdl.save(save_path)\n",
        "            print(f\"[Saved] Neural model → {save_path}\")\n",
        "\n",
        "        else:\n",
        "            save_path = os.path.join(output_path, f\"{key}.pkl\")\n",
        "            joblib.dump(mdl, save_path)\n",
        "            print(f\"[Saved] ML model → {save_path}\")\n",
        "\n",
        "    print(\"\\nAll models have been stored successfully!\")\n"
      ],
      "metadata": {
        "id": "N_zXIUtEDnzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "models_dict = {\n",
        "    \"lr_model\": lr_model,\n",
        "    \"svm\": svm_model,\n",
        "    \"random_forest\": rf_model,\n",
        "    \"xgboost\": xgb_model,\n",
        "    \"ffnn\": classifier\n",
        "}\n",
        "\n",
        "persist_models(models_dict)\n"
      ],
      "metadata": {
        "id": "KptJ9Z8LEey0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}